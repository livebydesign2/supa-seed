/**
 * AI-Powered Asset Generation System
 * Phase 5, Checkpoint E1 - Complete AI integration for intelligent asset and template generation
 */

import { OllamaClient, ollamaClient } from './ollama-client';
import { DomainPromptEngine, promptEngine as defaultPromptEngine, GenerationRequest, PromptContext } from './prompt-engine';
import { AIResponseCache, aiCache } from './response-cache';
import { SchemaInfo } from '../core/schema-adapter';
import { Template } from '../features/generation/template-engine';
import { Logger } from '../core/utils/logger';

export interface GenerationOptions {
  useCache?: boolean;
  cacheTimeout?: number;
  fallbackToFaker?: boolean;
  maxRetries?: number;
  qualityThreshold?: number;
  model?: string;
  temperature?: number;
  tags?: string[];
}

export interface AssetGenerationResult {
  success: boolean;
  data?: any;
  metadata: {
    source: 'ai' | 'cache' | 'fallback';
    model?: string;
    responseTime: number;
    quality?: number;
    tokensUsed?: number;
    cacheHit: boolean;
  };
  errors: string[];
  warnings: string[];
}

export interface TemplateRecommendation {
  template: Partial<Template>;
  confidence: number;
  reasoning: string;
  suggestedVariables: Array<{
    name: string;
    type: string;
    description: string;
    required: boolean;
    suggestedDefault?: any;
  }>;
}

export interface SchemaAnalysis {
  improvements: Array<{
    category: 'relationships' | 'indexes' | 'constraints' | 'seeding';
    title: string;
    description: string;
    priority: 'low' | 'medium' | 'high';
    implementation: string;
  }>;
  seedingStrategy: {
    order: string[];
    relationships: Array<{ from: string; to: string; type: string }>;
    estimatedTime: number;
  };
  qualityScore: number;
}

export class AIAssetGenerator {
  private ollama: OllamaClient;
  private promptEngine: DomainPromptEngine;
  private cache: AIResponseCache;

  constructor(
    ollama?: OllamaClient,
    promptEngine?: DomainPromptEngine,
    cache?: AIResponseCache
  ) {
    this.ollama = ollama || ollamaClient;
    this.promptEngine = promptEngine || defaultPromptEngine;
    this.cache = cache || aiCache;

    Logger.info('üöÄ AI Asset Generator initialized');
  }

  /**
   * Generate realistic seed data using AI
   */
  async generateSeedData(
    table: string,
    count: number,
    context: PromptContext,
    options?: GenerationOptions
  ): Promise<AssetGenerationResult> {
    const startTime = Date.now();
    
    try {
      Logger.info(`üéØ Generating ${count} records for ${table} using AI`);

      // Check if Ollama is available
      const health = await this.ollama.checkHealth();
      if (!health.connected && !options?.fallbackToFaker) {
        return {
          success: false,
          metadata: {
            source: 'ai',
            responseTime: Date.now() - startTime,
            cacheHit: false
          },
          errors: [`AI service unavailable: ${health.error}`],
          warnings: []
        };
      }

      // Generate prompt
      const { system, user, schema } = this.promptEngine.generateSeedDataPrompt({
        ...context,
        table
      }, count);

      // Check cache first
      if (options?.useCache !== false) {
        const cacheKey = `seed_data:${table}:${count}`;
        const cached = await this.cache.get(user, options?.model, [cacheKey, ...(options?.tags || [])]);
        
        if (cached) {
          Logger.info(`üí∞ Using cached seed data for ${table}`);
          return {
            success: true,
            data: cached,
            metadata: {
              source: 'cache',
              responseTime: Date.now() - startTime,
              cacheHit: true
            },
            errors: [],
            warnings: []
          };
        }
      }

      // Generate with AI if available
      if (health.connected) {
        try {
          const response = await this.ollama.generateJSON(user, system, schema);

          const responseTime = Date.now() - startTime;
          const quality = this.assessDataQuality(response, context);

          // Cache the response
          if (options?.useCache !== false && quality >= (options?.qualityThreshold || 70)) {
            await this.cache.set(
              user,
              response,
              health.recommendedModel || 'unknown',
              responseTime,
              {
                tags: [`seed_data`, table, ...(options?.tags || [])],
                quality,
                ttl: options?.cacheTimeout
              }
            );
          }

          Logger.info(`‚ú® Generated ${count} records for ${table} (quality: ${quality}%)`);

          return {
            success: true,
            data: response,
            metadata: {
              source: 'ai',
              model: health.recommendedModel,
              responseTime,
              quality,
              cacheHit: false
            },
            errors: [],
            warnings: quality < 80 ? [`Generated data quality is ${quality}%, consider manual review`] : []
          };

        } catch (error: any) {
          Logger.warn(`ü§ñ AI generation failed: ${error.message}`);
          
          if (options?.fallbackToFaker) {
            return await this.fallbackToFaker(table, count, context, startTime);
          }

          return {
            success: false,
            metadata: {
              source: 'ai',
              responseTime: Date.now() - startTime,
              cacheHit: false
            },
            errors: [`AI generation failed: ${error.message}`],
            warnings: []
          };
        }
      }

      // Fallback if AI not available
      if (options?.fallbackToFaker) {
        return await this.fallbackToFaker(table, count, context, startTime);
      }

      return {
        success: false,
        metadata: {
          source: 'ai',
          responseTime: Date.now() - startTime,
          cacheHit: false
        },
        errors: ['AI service unavailable and fallback disabled'],
        warnings: []
      };

    } catch (error: any) {
      Logger.error(`üö® Asset generation failed: ${error.message}`);
      
      return {
        success: false,
        metadata: {
          source: 'ai',
          responseTime: Date.now() - startTime,
          cacheHit: false
        },
        errors: [`Generation failed: ${error.message}`],
        warnings: []
      };
    }
  }

  /**
   * Generate intelligent template recommendations
   */
  async recommendTemplate(
    context: PromptContext,
    templateType: string,
    options?: GenerationOptions
  ): Promise<TemplateRecommendation | null> {
    try {
      Logger.info(`üé® Generating template recommendation for ${templateType}`);

      const health = await this.ollama.checkHealth();
      if (!health.connected) {
        Logger.warn('ü§ñ AI service unavailable for template recommendation');
        return null;
      }

      const { system, user } = this.promptEngine.generateTemplatePrompt(context, templateType);

      const response = await this.ollama.generateJSON(user, system);

      // Parse and validate template recommendation
      const recommendation = this.parseTemplateRecommendation(response, context, templateType);
      
      Logger.info(`üé® Generated template recommendation (confidence: ${recommendation.confidence}%)`);
      
      return recommendation;

    } catch (error: any) {
      Logger.error(`üö® Template recommendation failed: ${error.message}`);
      return null;
    }
  }

  /**
   * Analyze schema and suggest improvements
   */
  async analyzeSchema(
    schema: SchemaInfo,
    options?: GenerationOptions
  ): Promise<SchemaAnalysis | null> {
    try {
      Logger.info('üîç Analyzing schema with AI for improvements');

      const health = await this.ollama.checkHealth();
      if (!health.connected) {
        Logger.warn('ü§ñ AI service unavailable for schema analysis');
        return null;
      }

      // Check cache
      const cacheKey = `schema_analysis:${schema.makerkitVersion}:${schema.customTables.length}`;
      if (options?.useCache !== false) {
        const cached = await this.cache.get(JSON.stringify(schema), options?.model, [cacheKey]);
        if (cached) {
          Logger.info('üí∞ Using cached schema analysis');
          return cached;
        }
      }

      const { system, user } = this.promptEngine.generateSchemaAnalysisPrompt(schema);

      const response = await this.ollama.generateJSON(user, system);

      const analysis = this.parseSchemaAnalysis(response, schema);

      // Cache the analysis
      if (options?.useCache !== false) {
        await this.cache.set(
          JSON.stringify(schema),
          analysis,
          health.recommendedModel || 'unknown',
          0,
          {
            tags: ['schema_analysis', schema.frameworkType, cacheKey],
            quality: 90 // Schema analysis is generally high quality
          }
        );
      }

      Logger.info(`üîç Schema analysis complete (score: ${analysis.qualityScore}%)`);
      
      return analysis;

    } catch (error: any) {
      Logger.error(`üö® Schema analysis failed: ${error.message}`);
      return null;
    }
  }

  /**
   * Generate contextual field suggestions
   */
  async suggestFields(
    table: string,
    existingColumns: Array<{ name: string; type: string }>,
    domain: string,
    options?: GenerationOptions
  ): Promise<Array<{ name: string; type: string; description: string; required: boolean }>> {
    try {
      Logger.info(`üí° Generating field suggestions for ${table} in ${domain} domain`);

      const health = await this.ollama.checkHealth();
      if (!health.connected) {
        Logger.warn('ü§ñ AI service unavailable for field suggestions');
        return [];
      }

      const request: GenerationRequest = {
        type: 'field_names',
        context: {
          table,
          columns: existingColumns,
          domain
        }
      };

      const { system, user } = this.promptEngine.generatePrompt(request);

      const response = await this.ollama.generateJSON(user, system);

      return response.fields || [];

    } catch (error: any) {
      Logger.error(`üö® Field suggestion failed: ${error.message}`);
      return [];
    }
  }

  /**
   * Get AI service status
   */
  async getStatus(): Promise<{
    ai_available: boolean;
    model: string;
    cache_stats: any;
    recommendations: string[];
  }> {
    const health = await this.ollama.checkHealth();
    const cacheStats = this.cache.getStats();

    const recommendations: string[] = [];
    
    if (!health.connected) {
      recommendations.push('Install and start Ollama for AI-powered generation');
    }
    
    if (cacheStats.hitRate < 0.3) {
      recommendations.push('Consider enabling caching to improve performance');
    }
    
    if (health.availableModels.length === 0) {
      recommendations.push('Pull AI models using: ollama pull llama3.1:latest');
    }

    return {
      ai_available: health.connected,
      model: health.recommendedModel || 'none',
      cache_stats: cacheStats,
      recommendations
    };
  }

  /**
   * Clear AI cache
   */
  clearCache(criteria?: { tags?: string[]; model?: string }): number {
    return this.cache.clear(criteria);
  }

  /**
   * Private: Fallback to Faker.js generation
   */
  private async fallbackToFaker(
    table: string,
    count: number,
    context: PromptContext,
    startTime: number
  ): Promise<AssetGenerationResult> {
    Logger.info(`üé≤ Falling back to Faker.js for ${table}`);

    // This would integrate with existing faker-based generation
    // For now, return a placeholder structure
    const fakerData = {
      records: Array.from({ length: count }, (_, i) => ({
        id: i + 1,
        // Would generate actual faker data based on context
        generated_by: 'faker_fallback'
      })),
      metadata: {
        count,
        table,
        generated_at: new Date().toISOString()
      }
    };

    return {
      success: true,
      data: fakerData,
      metadata: {
        source: 'fallback',
        responseTime: Date.now() - startTime,
        cacheHit: false
      },
      errors: [],
      warnings: ['Using Faker.js fallback - data may be less contextual']
    };
  }

  /**
   * Private: Assess quality of generated data
   */
  private assessDataQuality(data: any, context: PromptContext): number {
    let score = 100;
    
    // Check if data structure is valid
    if (!data || !data.records) {
      return 0;
    }

    const records = data.records;
    if (!Array.isArray(records) || records.length === 0) {
      return 0;
    }

    // Check for required fields
    if (context.columns) {
      const requiredFields = context.columns.filter(c => !c.nullable).map(c => c.name);
      const firstRecord = records[0];
      
      for (const field of requiredFields) {
        if (!(field in firstRecord)) {
          score -= 20;
        }
      }
    }

    // Check for data diversity (simple check)
    if (records.length > 1) {
      const firstRecord = JSON.stringify(records[0]);
      const duplicateCount = records.filter(r => JSON.stringify(r) === firstRecord).length;
      
      if (duplicateCount > records.length * 0.5) {
        score -= 30; // Too many duplicates
      }
    }

    // Check for reasonable data types
    for (const record of records.slice(0, 3)) { // Check first 3 records
      for (const [key, value] of Object.entries(record)) {
        if (value === null || value === undefined) continue;
        
        // Basic type checking
        if (key.includes('email') && typeof value === 'string' && !value.includes('@')) {
          score -= 10;
        }
        if (key.includes('id') && typeof value !== 'number' && typeof value !== 'string') {
          score -= 10;
        }
      }
    }

    return Math.max(0, Math.min(100, score));
  }

  /**
   * Private: Parse template recommendation from AI response
   */
  private parseTemplateRecommendation(
    response: any,
    context: PromptContext,
    templateType: string
  ): TemplateRecommendation {
    // This would parse and validate the AI response
    // For now, return a basic structure
    return {
      template: {
        id: `ai-generated-${templateType}-${Date.now()}`,
        name: `AI Generated ${templateType} Template`,
        description: response.description || `Generated template for ${templateType}`,
        category: 'seeder',
        variables: response.variables || {},
        metadata: {
          created: new Date(),
          updated: new Date(),
          tags: ['ai-generated', templateType],
          version: '1.0.0',
          compatibility: {
            supaSeedVersion: '1.0.0'
          },
          files: response.files || []
        }
      },
      confidence: response.confidence || 75,
      reasoning: response.reasoning || 'AI generated based on schema analysis',
      suggestedVariables: response.suggestedVariables || []
    };
  }

  /**
   * Private: Parse schema analysis from AI response
   */
  private parseSchemaAnalysis(response: any, schema: SchemaInfo): SchemaAnalysis {
    return {
      improvements: response.suggestions || [],
      seedingStrategy: {
        order: response.seedingOrder || schema.customTables,
        relationships: response.relationships || [],
        estimatedTime: response.estimatedTime || schema.customTables.length * 30 // 30s per table estimate
      },
      qualityScore: response.qualityScore || 85
    };
  }
}

// Export singleton instance
export const aiAssetGenerator = new AIAssetGenerator();